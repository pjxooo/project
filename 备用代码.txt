# count = len(open('./ip.txt', 'rU').readlines())
# #生成随机行数
# hellonum = random.randrange(1, count, 1)
# #随机读取某行
# ranproxy = linecache.getline('./ip.txt', hellonum)
# chrome_options = webdriver.ChromeOptions()
# chrome_options.add_argument('--proxy-server=http://' + ranproxy)
# return driver
# filename = './ip.xlsx'
# workbook = load_workbook(filename)
# sheet = workbook.active
# cell_A = sheet.cell(1, 1).value
# chrome_options = webdriver.ChromeOptions()
# chrome_options.add_argument('--proxy-server=http://' + cell_A)
# driver = webdriver.Chrome(chrome_options=chrome_options)
# driver.get(url)
# sheet.delete_rows(1)
# workbook.save(filename)


验证demo
#!/usr/bin/env python3
# -*- coding:utf-8 -*-
import requests  # http客户端
import os  # 创建文件夹
from PIL import Image
import fateadm_api
def recog_img(x):
    os.makedirs('./easy_img/', exist_ok=True)
    IMAGE_URL = x
    def request_download():
        r = requests.get(IMAGE_URL)
        with open('./easy_img/img.png', 'wb') as f:
            f.write(r.content)
    try:
        request_download()
        # im = Image.open('./easy_img/img.png')
        # im.show()
        file_name = './easy_img/img.png'
        return fateadm_api.TestFunc(file_name)
    except:
        return print('download img error!')
# if __name__ == '__main__':
#     recog_img("/check/getCapImg?t=1563949964047")


redemo
# #!/usr/bin/env python
# # -*- coding:utf-8 -*-
# import re
# pattern1="cat"
# string="cat in house"
# print(re.search(pattern1, string))
# ptn=r"r[au]n"
# print(re.search(ptn,"dog rans to cat"))
# print(re.search(r"r[A-Z]n", "dog rans to cat"))
# print(re.search(r"r[a-z]n", "dog rans to cat"))
# print(re.search(r"r[0-9]n", "dog rans to cat"))
# print(re.search(r"r[0-9a-z]n", "dog rans to cat"))
#
# print(re.search(r"r\dn","runs r4n"))
# print(re.search(r"r\Dn","run rNn"))
# print(re.search(r"r\sn","r\nn r4n"))
# print(re.search(r"r.n","r[n"))
# string="""Y am here.
# I am running.
# """
# print(re.search(r"^I",string))
# print(re.search(r"^I",string,flags=re.M))
#
# match=re.search(r"(\d+),Date:(.+)", "ID:43085385,Date:Feb/12/2017")
# print(match.group())
# print(match.group(1))
# print(match.group(2))
#
# match = re.search(r"(?P<id>\d+),Date:(?P<date>.+)", "ID:43085385,Date:Feb/12/2017")
# print(match.group('id'))
# print(match.group('date'))
#
# print(re.findall(r"r[ua]n","run ran"))
# print(re.findall(r"(run|ran)","run ran ren"))
#
# print(re.sub(r"r[au]ns","catch","dog runs to cat"))
# print(re.split(r"/^.*[:：](\d+)$/", "a:c:b:s:f;e"))
#
# from urllib.request import urlopen
# html=urlopen("https://xin.baidu.com/detail/compinfo?pid=xlTM-TogKuTwHmK7stNTwgdIEaalWZqZTwmd").read().decode('utf-8')
# res=re.findall(r"<title>.+?</title>",html)
# print("\nPage title is:",res[0])
# res1=re.findall(r"<p>.*?</p>", html, flags=re.DOTALL)
# print("\nPage title is:", res1[0])
# res2=re.findall(r'href="(.*?)"', html)
# print("all links:", res2)
#
#
import re
from bs4 import BeautifulSoup
from selenium import webdriver
option=webdriver.ChromeOptions()
option.add_argument('--headless')
option.add_argument('--no-sandbox')
option.add_argument('--start-maximized')
driver = webdriver.Chrome(chrome_options=option)

url = 'https://xin.baidu.com/s?q=baidu&t=0'
driver.get(url)
driver.forward()
html = driver.page_source
# soup = BeautifulSoup(html, features='lxml')
a = re.findall(r"抱歉",html)
print(len(a))
if len(a) == 0:
    print("有公司")
else:
    print("没有公司")



博客园 验证demo
from PIL import Image
from pytesseract import *
from fnmatch import fnmatch
from queue import Queue
import cv2
import os

def clear_border(img,img_name):
  '''去除边框
  '''

  filename = './out_img/' + img_name.split('.')[0] + '-clearBorder.jpg'
  h, w = img.shape[:2]
  for y in range(0, w):
    for x in range(0, h):
      if y < 4 or y > w - 4:
        img[x, y] = 255
      if x < 4 or x > h - 4:
        img[x, y] = 255

  cv2.imwrite(filename, img)
  return img


def interference_line(img, img_name):
  '''
  干扰线降噪
  '''

  filename = './out_img/' + img_name.split('.')[0] + '-interferenceline.jpg'
  h, w = img.shape[:2]
  # ！！！opencv矩阵点是反的
  # img[1,2] 1:图片的高度，2：图片的宽度
  for y in range(1, w - 1):
    for x in range(1, h - 1):
      count = 0
      if img[x, y - 1] > 245:
        count = count + 1
      if img[x, y + 1] > 245:
        count = count + 1
      if img[x - 1, y] > 245:
        count = count + 1
      if img[x + 1, y] > 245:
        count = count + 1
      if count > 2:
        img[x, y] = 255
  cv2.imwrite(filename, img)
  return img

def interference_point(img,img_name, x = 0, y = 0):
    """点降噪
    9邻域框,以当前点为中心的田字框,黑点个数
    :param x:
    :param y:
    :return:
    """
    filename = './out_img/' + img_name.split('.')[0] + '-interferencePoint.jpg'
    # todo 判断图片的长宽度下限
    # 当前像素点的值
    cur_pixel = img[x,y]
    height,width = img.shape[:2]

    for y in range(0, width - 1):
      for x in range(0, height - 1):
        if y == 0:  # 第一行
            if x == 0:  # 左上顶点,4邻域
                # 中心点旁边3个点
                sum = int(cur_pixel) \
                      + int(img[x, y + 1]) \
                      + int(img[x + 1, y]) \
                      + int(img[x + 1, y + 1])
                if sum <= 2 * 245:
                  img[x, y] = 0
            elif x == height - 1:  # 右上顶点
                sum = int(cur_pixel) \
                      + int(img[x, y + 1]) \
                      + int(img[x - 1, y]) \
                      + int(img[x - 1, y + 1])
                if sum <= 2 * 245:
                  img[x, y] = 0
            else:  # 最上非顶点,6邻域
                sum = int(img[x - 1, y]) \
                      + int(img[x - 1, y + 1]) \
                      + int(cur_pixel) \
                      + int(img[x, y + 1]) \
                      + int(img[x + 1, y]) \
                      + int(img[x + 1, y + 1])
                if sum <= 3 * 245:
                  img[x, y] = 0
        elif y == width - 1:  # 最下面一行
            if x == 0:  # 左下顶点
                # 中心点旁边3个点
                sum = int(cur_pixel) \
                      + int(img[x + 1, y]) \
                      + int(img[x + 1, y - 1]) \
                      + int(img[x, y - 1])
                if sum <= 2 * 245:
                  img[x, y] = 0
            elif x == height - 1:  # 右下顶点
                sum = int(cur_pixel) \
                      + int(img[x, y - 1]) \
                      + int(img[x - 1, y]) \
                      + int(img[x - 1, y - 1])

                if sum <= 2 * 245:
                  img[x, y] = 0
            else:  # 最下非顶点,6邻域
                sum = int(cur_pixel) \
                      + int(img[x - 1, y]) \
                      + int(img[x + 1, y]) \
                      + int(img[x, y - 1]) \
                      + int(img[x - 1, y - 1]) \
                      + int(img[x + 1, y - 1])
                if sum <= 3 * 245:
                  img[x, y] = 0
        else:  # y不在边界
            if x == 0:  # 左边非顶点
                sum = int(img[x, y - 1]) \
                      + int(cur_pixel) \
                      + int(img[x, y + 1]) \
                      + int(img[x + 1, y - 1]) \
                      + int(img[x + 1, y]) \
                      + int(img[x + 1, y + 1])

                if sum <= 3 * 245:
                  img[x, y] = 0
            elif x == height - 1:  # 右边非顶点
                sum = int(img[x, y - 1]) \
                      + int(cur_pixel) \
                      + int(img[x, y + 1]) \
                      + int(img[x - 1, y - 1]) \
                      + int(img[x - 1, y]) \
                      + int(img[x - 1, y + 1])

                if sum <= 3 * 245:
                  img[x, y] = 0
            else:  # 具备9领域条件的
                sum = int(img[x - 1, y - 1]) \
                      + int(img[x - 1, y]) \
                      + int(img[x - 1, y + 1]) \
                      + int(img[x, y - 1]) \
                      + int(cur_pixel) \
                      + int(img[x, y + 1]) \
                      + int(img[x + 1, y - 1]) \
                      + int(img[x + 1, y]) \
                      + int(img[x + 1, y + 1])
                if sum <= 4 * 245:
                  img[x, y] = 0
    cv2.imwrite(filename,img)
    return img

def _get_dynamic_binary_image(filedir, img_name):
  '''
  自适应阀值二值化
  '''

  filename =   './out_img/' + img_name.split('.')[0] + '-binary.jpg'
  img_name = filedir + '/' + img_name
  print('.....' + img_name)
  im = cv2.imread(img_name)
  im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)

  th1 = cv2.adaptiveThreshold(im, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 21, 1)
  cv2.imwrite(filename, th1)
  return th1

def _get_static_binary_image(img, threshold = 140):
  '''
  手动二值化
  '''

  img = Image.open(img)
  img = img.convert('L')
  pixdata = img.load()
  w, h = img.size
  for y in range(h):
    for x in range(w):
      if pixdata[x, y] < threshold:
        pixdata[x, y] = 0
      else:
        pixdata[x, y] = 255

  return img


def cfs(im,x_fd,y_fd):
  '''用队列和集合记录遍历过的像素坐标代替单纯递归以解决cfs访问过深问题
  '''
  xaxis=[]
  yaxis=[]
  visited =set()
  q = Queue()
  q.put((x_fd, y_fd))
  visited.add((x_fd, y_fd))
  offsets=[(1, 0), (0, 1), (-1, 0), (0, -1)]
  # 四邻域
  while not q.empty():
      x,y=q.get()

      for xoffset,yoffset in offsets:
          x_neighbor,y_neighbor = x+xoffset,y+yoffset

          if (x_neighbor,y_neighbor) in (visited):
              continue  # 已经访问过了

          visited.add((x_neighbor, y_neighbor))

          try:
              if im[x_neighbor, y_neighbor] == 0:
                  xaxis.append(x_neighbor)
                  yaxis.append(y_neighbor)
                  q.put((x_neighbor,y_neighbor))

          except IndexError:
              pass
  # print(xaxis)
  if (len(xaxis) == 0 | len(yaxis) == 0):
    xmax = x_fd + 1
    xmin = x_fd
    ymax = y_fd + 1
    ymin = y_fd

  else:
    xmax = max(xaxis)
    xmin = min(xaxis)
    ymax = max(yaxis)
    ymin = min(yaxis)
    #ymin,ymax=sort(yaxis)

  return ymax,ymin,xmax,xmin

def detectFgPix(im,xmax):
  '''搜索区块起点
  '''

  h,w = im.shape[:2]
  for y_fd in range(xmax+1,w):
      for x_fd in range(h):
          if im[x_fd,y_fd] == 0:
              return x_fd,y_fd

def CFS(im):
  '''切割字符位置
  '''

  zoneL=[]#各区块长度L列表
  zoneWB=[]#各区块的X轴[起始，终点]列表
  zoneHB=[]#各区块的Y轴[起始，终点]列表

  xmax=0#上一区块结束黑点横坐标,这里是初始化
  for i in range(10):

      try:
          x_fd,y_fd = detectFgPix(im,xmax)
          # print(y_fd,x_fd)
          xmax,xmin,ymax,ymin=cfs(im,x_fd,y_fd)
          L = xmax - xmin
          H = ymax - ymin
          zoneL.append(L)
          zoneWB.append([xmin,xmax])
          zoneHB.append([ymin,ymax])

      except TypeError:
          return zoneL,zoneWB,zoneHB

  return zoneL,zoneWB,zoneHB


def cutting_img(im,im_position,img,xoffset = 1,yoffset = 1):
  filename =  './out_img/' + img.split('.')[0]
  # 识别出的字符个数
  im_number = len(im_position[1])
  # 切割字符
  for i in range(im_number):
    im_start_X = im_position[1][i][0] - xoffset
    im_end_X = im_position[1][i][1] + xoffset
    im_start_Y = im_position[2][i][0] - yoffset
    im_end_Y = im_position[2][i][1] + yoffset
    cropped = im[im_start_Y:im_end_Y, im_start_X:im_end_X]
    cv2.imwrite(filename + '-cutting-' + str(i) + '.jpg',cropped)



def main():
  filedir = './easy_img'
  for file in os.listdir(filedir):
    if fnmatch(file, '*.jpg'):
      img_name = file
      # 自适应阈值二值化
      im = _get_dynamic_binary_image(filedir, img_name)
      # 去除边框
      im = clear_border(im,img_name)
      # 对图片进行干扰线降噪
      im = interference_line(im,img_name)
      # 对图片进行点降噪
      im = interference_point(im,img_name)
      # 切割的位置
      im_position = CFS(im)
      maxL = max(im_position[0])
      minL = min(im_position[0])
      # 如果有粘连字符，如果一个字符的长度过长就认为是粘连字符，并从中间进行切割
      if(maxL > minL + minL * 0.7):
        maxL_index = im_position[0].index(maxL)
        minL_index = im_position[0].index(minL)
        # 设置字符的宽度
        im_position[0][maxL_index] = maxL // 2
        im_position[0].insert(maxL_index + 1, maxL // 2)
        # 设置字符X轴[起始，终点]位置
        im_position[1][maxL_index][1] = im_position[1][maxL_index][0] + maxL // 2
        im_position[1].insert(maxL_index + 1, [im_position[1][maxL_index][1] + 1, im_position[1][maxL_index][1] + 1 + maxL // 2])
        # 设置字符的Y轴[起始，终点]位置
        im_position[2].insert(maxL_index + 1, im_position[2][maxL_index])
      # 切割字符，要想切得好就得配置参数，通常 1 or 2 就可以
      cutting_img(im,im_position,img_name,1,1)
      # 识别验证码
      cutting_img_num = 0
      for file in os.listdir('./out_img'):
        str_img = ''
        if fnmatch(file, '%s-cutting-*.jpg' % img_name.split('.')[0]):
          cutting_img_num += 1
      for i in range(cutting_img_num):
        try:
          file = './out_img/%s-cutting-%s.jpg' % (img_name.split('.')[0], i)
          # 识别验证码
          str_img = str_img + image_to_string(Image.open(file), lang='eng', config='-psm 10')
          #单个字符是10，一行文本是7
        except Exception as err:
          pass
      print('切图:'+str(cutting_img_num))
      print('识别为:'+str_img)
if __name__ == '__main__':
  main()



beautifulsoup  demo
#!/usr/bin/env python
# -*- coding:utf-8 -*-
import lxml as lxml
from selenium.webdriver.chrome.options import Options
chrome_options = Options()
chrome_options.add_argument("--headless")
from selenium import webdriver
import time
import re
from selenium.webdriver.support.expected_conditions import _find_elements
from bs4 import BeautifulSoup
from urllib.request import urlopen
driver = webdriver.Chrome()
html = "https://xin.baidu.com"
driver.get(html)
driver.forward()
driver.find_element_by_class_name("search-text").send_keys("中企志诚")
driver.find_element_by_class_name("search-btn").click()
driver.find_elements_by_class_name("zx-list-item-url")[0].click()
# htmll = driver.page_source
# soup = BeautifulSoup(htmll, features='lxml')
# print(soup.find_all('span', {'class':"zx-ent-pre-title"})[0].get_text())
# address = soup.find('a',{'href':"/detail/compinfo?pid=xlTM-TogKuTwHmK7stNTwgdIEaalWZqZTwmd"})
# print("公司名称",address['title'])  # 查a标签的href值
# print(soup.find_all('span', {'class': re.compile('.*?middle')})[1].get_text())
# print(soup.find_all('span', {'class': re.compile('.*?short')})[0].get_text())
# print(soup.find_all('span', {'class': re.compile('.*?long')})[0].get_text())
# print("经营范围：",soup.find_all('span', {'class': 'scope-text-content'})[0].get_text())
# driver.forward()
# html2 = driver.page_source
# print(html2)
# zhichenga = soup.find('a', {'title': re.compile('中企志诚')})
# zhichenghref=zhichenga['href']
# html2=urlopen("https://xin.baidu.com"+zhichenghref).read().decode('utf-8')
# soup2=BeautifulSoup(html2, features='lxml')
# driver.find_elements_by_class_name("tab-item-name")[0].click()
# html3=driver.page_source
# print(html3)

# # print(soup.find_all('em'))
# print("\n", soup.p)
# print("\n", soup.div)
# # 得到所有a的标签
# all_href = soup.find_all(attrs = {"data-click":""})
# print(all_href)
# # 得到a标签的href这个属性
# for l in soup.find_all('a'):
#      print(l.get('href'))
# # 抓图片
# print("找图片")
# img_links = soup.find_all("img", {"src": re.compile('.*?\.png')})
# print(img_links)
# course_links=soup.find_all('a',{'href':re.compile('http://xin.baidu.*')})
# print(course_links)
# print("读title")
# print(soup.find('title').get_text())
# print("读百度信用代码")
# spans=soup.find_all('span', {'id': "baiducode"})
# print(spans)
# tddemo=soup.find_all('td', {'class': "table-regCapital-lable"})
# for t in tddemo:
#     print(t.get_text)





终极救命demo
#!/usr/bin/env python
# -*- coding:utf-8 -*-
import linecache
from openpyxl import *
import re
import time
import pymysql
import random
from bs4 import BeautifulSoup
from Company import Company
from selenium import webdriver
from Shareholder import Shareholder
import threading
import datetime
import test
# 数据库连接

conn = pymysql.connect('192.168.2.253', 'test', '123456')
conn.select_db('cloud_account_system')
# 获取游标
cur = conn.cursor()
# 爬取数据的网站头
url = 'https://xin.baidu.com'
headers = {
        'Accept-Encoding': 'gzip, deflate',
        'Accept-Language': 'zh-CN,zh;q=0.8',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Cache-Control': 'max-age=0',
        'Connection': 'keep-alive',
    }
def delete_row():
    filename = './exx.xlsx'
    workbook = load_workbook(filename)
    sheet = workbook.active
    sheet.delete_rows(1)
    workbook.save(filename)
    open_excel()
def open_xin_web(url):
    # 打开谷歌浏览器
    driver = webdriver.Chrome()
    driver.get(url)
    return driver
def company_info(string):
    driver = open_xin_web(url)
    # driver = test.test_driver()
    driver.refresh()
    time.sleep(3)
    driver.find_element_by_class_name("search-text").send_keys(string)
    driver.find_element_by_class_name("search-btn").click()
    time.sleep(3)
    try:
        driver.find_elements_by_xpath(".//*[@class='zx-ent-logo']")[0].click()
    except Exception as err:
        print("没有找到该公司")
        delete_row()
    time.sleep(3)
    # 切换当前页面标签
    driver.switch_to.window(driver.window_handles[1])
    # 获取新标签url
    rul = driver.current_url
    driver.get(rul)
    driver.forward()
    html = driver.page_source
    soup = BeautifulSoup(html, features='lxml')
    # 企业基本信息
    company = Company()
    company.setCompanyId(random.randint(10000000, 100000000))
    time.sleep(5)
    cname = driver.find_element_by_xpath(".//*[@class='zx-detail-company'][1]/h2")
    company.setCompanyName(str(cname.text))
    cweb = driver.find_element_by_xpath(".//*[@class='zx-detail-company'][1]/div[2]/div[5]")
    ccweb = ''.join(re.findall(r"[^官网：]", cweb.text))
    company.setCompanyWebsite(str(ccweb))
    cphone = driver.find_element_by_xpath(".//*[@class='zx-detail-company'][1]/div[2]/div[3]")
    ccphone = ''.join(re.findall(r"[^电话：]", cphone.text))
    company.setCompanyPhone(str(ccphone))
    cemail = driver.find_element_by_xpath(".//*[@class='zx-detail-company'][1]/div[2]/div[4]")
    ccemail = ''.join(re.findall(r"[^邮箱：]", cemail.text))
    company.setCompanyEmail(str(ccemail))
    caddress = driver.find_element_by_xpath(".//*[@class='zx-detail-company'][1]/div[2]/div[6]")
    ccaddress = ''.join(re.findall(r"[^地址：]", caddress.text))
    company.setCompanyAddress(str(ccaddress))
    cinfo = driver.find_element_by_xpath(".//*[@class='zx-detail-company'][1]/div[2]/div[7]/p/span[1]")
    company.setCompanyInfo(str(cinfo.text))
    tds = driver.find_elements_by_xpath(".//*[@class='zx-detail-basic-table']/tbody/tr/td")
    capital = tds[1].text
    ccapital = ''.join(re.findall(r"[0-9]", capital))
    company.setRegisteredCapital(str(ccapital))
    company.setContributedMapital(str(tds[3].text))
    company.setLegalRepresentative(str(tds[5].text))
    company.setManagementForms(str(tds[7].text))
    company.setUsedName(str(tds[9].text))
    company.setIndustryInvolved(str(tds[11].text))
    company.setUnifiedSocialCreditCode(str(tds[13].text))
    company.setTaxpayerRegistrationNumber(str(tds[15].text))
    company.setBusinessRegistrationNumber(str(tds[17].text))
    company.setOrganizingInstitutionBarCode(str(tds[19].text))
    company.setRegistrationAuthority(str(tds[21].text))
    company.setDateOfEstablishment(str(tds[23].text))
    company.setTypeOfEnterprise(str(tds[25].text))
    company.setBusinessTerm(str(tds[27].text))
    company.setAdministrativeDivision(str(tds[29].text))
    company.setAnnualInspectionDate(str(tds[31].text))
    ra = tds[33].text
    rea = ''.join(re.findall(r"(.*)查看地图", ra))
    company.setRegisteredAddress(str(rea))
    company.setBusinessScope(str(tds[35].text))
    time.sleep(3)
    sql_company_insert = "insert into tbl_sycs_python_company(companyId,companyName,companyWebsite,companyPhone,companyEmail,companyAddress,companyInfo,registeredCapital,contributedMapital,legalRepresentative,managementForms,usedName,industryInvolved,unifiedSocialCreditCode,taxpayerRegistrationNumber,businessRegistrationNumber,organizingInstitutionBarCode,registrationAuthority,dateOfEstablishment,typeOfEnterprise,businessTerm,administrativeDivision,annualInspectionDate,registeredAddress,businessScope) values (%(companyId)s, %(companyName)s, %(companyWebsite)s, %(companyPhone)s, %(companyEmail)s, %(companyAddress)s, %(companyInfo)s, %(registeredCapital)s, %(contributedMapital)s,%(legalRepresentative)s,%(managementForms)s,%(usedName)s,%(industryInvolved)s,%(unifiedSocialCreditCode)s,%(taxpayerRegistrationNumber)s,%(businessRegistrationNumber)s,%(organizingInstitutionBarCode)s,%(registrationAuthority)s,%(dateOfEstablishment)s,%(typeOfEnterprise)s,%(businessTerm)s,%(administrativeDivision)s,%(annualInspectionDate)s,%(registeredAddress)s,%(businessScope)s)"
    company_message = {"companyId": company.getCompanyId(),
               "companyName": company.getCompanyName(),
               "companyWebsite": company.getCompanyWebsite(),
               "companyPhone": company.getCompanyPhone(),
               "companyEmail": company.getCompanyEmail(),
               "companyAddress": company.getCompanyAddress(),
               "companyInfo": company.getCompanyInfo(),
               "registeredCapital": company.getRegisteredCapital(),
               "contributedMapital": company.getContributedMapital(),
               "legalRepresentative": company.getLegalRepresentative(),
               "managementForms": company.getManagementForms(),
               "usedName": company.getUsedName(),
               "industryInvolved": company.getIndustryInvolved(),
               "unifiedSocialCreditCode": company.getUnifiedSocialCreditCode(),
               "taxpayerRegistrationNumber": company.getTaxpayerRegistrationNumber(),
               "businessRegistrationNumber": company.getBusinessRegistrationNumber(),
               "organizingInstitutionBarCode": company.getOrganizingInstitutionBarCode(),
               "registrationAuthority": company.getRegistrationAuthority(),
               "dateOfEstablishment": company.getDateOfEstablishment(),
               "typeOfEnterprise": company.getTypeOfEnterprise(),
               "businessTerm": company.getBusinessTerm(),
               "administrativeDivision":company.getAdministrativeDivision(),
               "annualInspectionDate": company.getAnnualInspectionDate(),
               "registeredAddress": company.getRegisteredAddress(),
               "businessScope": company.getBusinessScope()
               }
    cur.execute(sql_company_insert, company_message)
    conn.commit()
    # 股东信息
    shareholer = Shareholder()
    shareholerList = list()
    # 判断股东类型
    try:
        c = soup.find("table", {"class": "zx-detail-table"}).tbody
        time.sleep(5)
        a = re.findall(r"<[a]", str(c))
        sp = re.findall(r"<[s]", str(c))
        la = len(a)
        lp = len(sp)
        table = driver.find_element_by_xpath(".//*[@class='zx-detail-table'][1]/tbody")
        table_rows = table.find_elements_by_tag_name('tr')
        for sh in range(len(table_rows)):
            time.sleep(5)
            s = driver.find_elements_by_xpath(".//*[@class='zx-detail-table'][1]/tbody/tr[%s]/td"%(sh+1))
            try:
                if la == 0 and lp > 0:
                    sname = driver.find_element_by_xpath(".//*[@class='zx-detail-table'][1]/tbody/tr[%s]/td[2]/span"%(sh+1))
                    shareholer.setShareHolder(str(sname.text))
                elif la > 0 and lp == 0:
                    sname = driver.find_element_by_xpath(".//*[@class='zx-detail-table'][1]/tbody/tr[%s]/td[2]/a[1]"%(sh+1))
                    shareholer.setShareHolder(str(sname.text))
                elif la > 0 and lp > 0:
                    sname = driver.find_element_by_xpath(".//*[@class='zx-detail-table'][1]/tbody/tr[%s]/td[2]" % (sh + 1))
                    ids = list(sname.text)
                    news_ids = []
                    for id in ids:
                        if id not in news_ids:
                            news_ids.append(id)
                    str2 = ''
                    sname = str2.join(news_ids)
                    shareholer.setShareHolder(sname)
            except Exception as err:
                pass
            time.sleep(3)
            shareholer.setShareholdingRatio(str(s[2].text))
            shareholer.setSubscribedCapitalContribution(str(s[3].text))
            shareholer.setActualCapitalContribution(str(s[4].text))
            shareholer.setCompanyId(company.getCompanyId())
            shareholerList.append(shareholer)
            sql_shareholder_insert = "insert into tbl_sycs_python_shareholder(shareHolder,shareholdingRatio,subscribedCapitalContribution,actualCapitalContribution,companyId) values (%(shareHolder)s, %(shareholdingRatio)s, %(subscribedCapitalContribution)s, %(actualCapitalContribution)s, %(companyId)s)"
            shareholer_message = {"shareHolder": shareholerList[sh].getShareHolder(), "shareholdingRatio": shareholerList[sh].getShareholdingRatio(), "subscribedCapitalContribution": shareholerList[sh].getSubscribedCapitalContribution(), "actualCapitalContribution": shareholerList[sh].getActualCapitalContribution(), "companyId": shareholerList[sh].getCompangId()}
            cur.execute(sql_shareholder_insert, shareholer_message)
            conn.commit()
    except Exception as err:
        pass
def open_excel():
    filename = './exx.xlsx'
    workbook = load_workbook(filename)
    sheet = workbook.active
    rows = sheet.max_row
    for i in range(1, rows+1):
        cell_A = sheet.cell(1, 1).value
        company_info(cell_A)
        print(cell_A)
        sheet.delete_rows(1)
        workbook.save(filename)

if __name__ == '__main__':
    open_excel()
cur.close()
conn.close()



隧道代理
from selenium import webdriver
import string
import zipfile
import time
# 代理服务器
proxyHost = "http-pro.abuyun.com"
proxyPort = "9010"
# 代理隧道验证信息
proxyUser = "HT266541P5Y73V7P"
proxyPass = "121FA069C5B53F58"
def create_proxy_auth_extension(proxy_host, proxy_port,
                                proxy_username, proxy_password,
                                scheme='http', plugin_path=None):
    if plugin_path is None:
        plugin_path = r'D:/{}_{}@http-pro.abuyun.com_9010.zip'.format(proxy_username, proxy_password)
    manifest_json = """
        {
            "version": "1.0.0",
            "manifest_version": 2,
            "name": "Abuyun Proxy",
            "permissions": [
                "proxy",
                "tabs",
                "unlimitedStorage",
                "storage",
                "<all_urls>",
                "webRequest",
                "webRequestBlocking"
            ],
            "background": {
                "scripts": ["background.js"]
            },
            "minimum_chrome_version":"22.0.0"
        }
        """

    background_js = string.Template(
        """
        var config = {
            mode: "fixed_servers",
            rules: {
                singleProxy: {
                    scheme: "${scheme}",
                    host: "${host}",
                    port: parseInt(${port})
                },
                bypassList: ["foobar.com"]
            }
          };

        chrome.proxy.settings.set({value: config, scope: "regular"}, function() {});

        function callbackFn(details) {
            return {
                authCredentials: {
                    username: "${username}",
                    password: "${password}"
                }
            };
        }

        chrome.webRequest.onAuthRequired.addListener(
            callbackFn,
            {urls: ["<all_urls>"]},
            ['blocking']
        );
        """
    ).substitute(
        host=proxy_host,
        port=proxy_port,
        username=proxy_username,
        password=proxy_password,
        scheme=scheme,
    )
    with zipfile.ZipFile(plugin_path, 'w') as zp:
        zp.writestr("manifest.json", manifest_json)
        zp.writestr("background.js", background_js)
    return plugin_path
def test_driver():
    proxy_auth_plugin_path = create_proxy_auth_extension(
    proxy_host=proxyHost,
    proxy_port=proxyPort,
    proxy_username=proxyUser,
    proxy_password=proxyPass)
    option = webdriver.ChromeOptions()
    option.add_argument("--start-maximized")
    option.add_extension(proxy_auth_plugin_path)
    driver = webdriver.Chrome(chrome_options=option)
    driver.get('https://xin.baidu.com')
    time.sleep(2)
    driver.refresh()
    return driver
# driver.get("http://xin.baidu.com")
# driver.find_element_by_class_name("search-text").send_keys("百度")
# driver.find_element_by_class_name("search-btn").click()
# driver.find_elements_by_xpath(".//*[@class='zx-ent-logo']")[0].click()
# open_excel()
# driver.get("http://test.abuyun.com")



斐斐打码
# coding=utf-8
import os,sys
import hashlib
import time
import json
import requests

FATEA_PRED_URL  = "http://pred.fateadm.com"

def LOG(log):
    # 不需要测试时，注释掉日志就可以了
    log = None

class TmpObj():
    def __init__(self):
        self.value  = None

class Rsp():
    def __init__(self):
        self.ret_code   = -1
        self.cust_val   = 0.0
        self.err_msg    = "succ"
        self.pred_rsp   = TmpObj()

    def ParseJsonRsp(self, rsp_data):
        if rsp_data is None:
            self.err_msg     = "http request failed, get rsp Nil data"
            return
        jrsp                = json.loads( rsp_data)
        self.ret_code       = int(jrsp["RetCode"])
        self.err_msg        = jrsp["ErrMsg"]
        self.request_id     = jrsp["RequestId"]
        if self.ret_code == 0:
            rslt_data   = jrsp["RspData"]
            if rslt_data is not None and rslt_data != "":
                jrsp_ext    = json.loads( rslt_data)
                if "cust_val" in jrsp_ext:
                    data        = jrsp_ext["cust_val"]
                    self.cust_val   = float(data)
                if "result" in jrsp_ext:
                    data        = jrsp_ext["result"]
                    self.pred_rsp.value     = data

def CalcSign(pd_id, passwd, timestamp):
    md5     = hashlib.md5()
    md5.update((timestamp + passwd).encode())
    csign   = md5.hexdigest()

    md5     = hashlib.md5()
    md5.update((pd_id + timestamp + csign).encode())
    csign   = md5.hexdigest()
    return csign

def CalcCardSign(cardid, cardkey, timestamp, passwd):
    md5     = hashlib.md5()
    md5.update(passwd + timestamp + cardid + cardkey)
    return md5.hexdigest()

def HttpRequest(url, body_data, img_data=""):
    rsp = Rsp()
    post_data = body_data
    files = {
        'img_data':('img_data', img_data)
    }
    header = {
            'User-Agent': 'Mozilla/5.0',
            }
    rsp_data = requests.post(url, post_data, files=files, headers=header)
    rsp.ParseJsonRsp(rsp_data.text)
    return rsp

class FateadmApi():
    # API接口调用类
    # 参数（appID，appKey，pdID，pdKey）
    def __init__(self, app_id, app_key, pd_id, pd_key):
        self.app_id = app_id
        if app_id is None:
            self.app_id = ""
        self.app_key = app_key
        self.pd_id = pd_id
        self.pd_key = pd_key
        self.host = FATEA_PRED_URL

    def SetHost(self, url):
        self.host = url

    #
    # 查询余额
    # 参数：无
    # 返回值：
    #   rsp.ret_code：正常返回0
    #   rsp.cust_val：用户余额
    #   rsp.err_msg：异常时返回异常详情
    #
    def QueryBalc(self):
        tm      = str( int(time.time()))
        sign    = CalcSign( self.pd_id, self.pd_key, tm)
        param   = {
                "user_id": self.pd_id,
                "timestamp":tm,
                "sign":sign
                }
        url     = self.host + "/api/custval"
        rsp     = HttpRequest(url, param)
        if rsp.ret_code == 0:
            LOG("query succ ret: {} cust_val: {} rsp: {} pred: {}".format( rsp.ret_code, rsp.cust_val, rsp.err_msg, rsp.pred_rsp.value))
        else:
            LOG("query failed ret: {} err: {}".format( rsp.ret_code, rsp.err_msg.encode('utf-8')))
        return rsp

    #
    # 查询网络延迟
    # 参数：pred_type:识别类型
    # 返回值：
    #   rsp.ret_code：正常返回0
    #   rsp.err_msg： 异常时返回异常详情
    #
    def QueryTTS(self, pred_type):
        tm          = str( int(time.time()))
        sign        = CalcSign( self.pd_id, self.pd_key, tm)
        param       = {
                "user_id": self.pd_id,
                "timestamp":tm,
                "sign":sign,
                "predict_type":pred_type,
                }
        if self.app_id != "":
            #
            asign       = CalcSign(self.app_id, self.app_key, tm)
            param["appid"]     = self.app_id
            param["asign"]      = asign
        url     = self.host + "/api/qcrtt"
        rsp     = HttpRequest(url, param)
        if rsp.ret_code == 0:
            LOG("query rtt succ ret: {} request_id: {} err: {}".format( rsp.ret_code, rsp.request_id, rsp.err_msg))
        else:
            LOG("predict failed ret: {} err: {}".format( rsp.ret_code, rsp.err_msg.encode('utf-8')))
        return rsp

    #
    # 识别验证码
    # 参数：pred_type:识别类型  img_data:图片的数据
    # 返回值：
    #   rsp.ret_code：正常返回0
    #   rsp.request_id：唯一订单号
    #   rsp.pred_rsp.value：识别结果
    #   rsp.err_msg：异常时返回异常详情
    #
    def Predict(self, pred_type, img_data, head_info = ""):
        tm = str(int(time.time()))
        sign = CalcSign(self.pd_id, self.pd_key, tm)
        param = {
                "user_id": self.pd_id,
                "timestamp": tm,
                "sign": sign,
                "predict_type": pred_type,
                "up_type": "mt"
                }
        if head_info is not None or head_info != "":
            param["head_info"] = head_info
        if self.app_id != "":
            #
            asign = CalcSign(self.app_id, self.app_key, tm)
            param["appid"] = self.app_id
            param["asign"] = asign
        url = self.host + "/api/capreg"
        files = img_data
        rsp = HttpRequest(url, param, files)
        if rsp.ret_code == 0:
            LOG("predict succ ret: {} request_id: {} pred: {} err: {}".format(rsp.ret_code, rsp.request_id, rsp.pred_rsp.value, rsp.err_msg))
        else:
            LOG("predict failed ret: {} err: {}".format(rsp.ret_code, rsp.err_msg))
            if rsp.ret_code == 4003:
                #lack of money
                LOG("cust_val <= 0 lack of money, please charge immediately")
        return rsp

    #
    # 从文件进行验证码识别
    # 参数：pred_type;识别类型  file_name:文件名
    # 返回值：
    #   rsp.ret_code：正常返回0
    #   rsp.request_id：唯一订单号
    #   rsp.pred_rsp.value：识别结果
    #   rsp.err_msg：异常时返回异常详情
    #
    def PredictFromFile(self, pred_type, file_name, head_info=""):
        with open(file_name, "rb") as f:
            data = f.read()
        return self.Predict(pred_type, data, head_info=head_info)

    #
    # 识别失败，进行退款请求
    # 参数：request_id：需要退款的订单号
    # 返回值：
    #   rsp.ret_code：正常返回0
    #   rsp.err_msg：异常时返回异常详情
    #
    # 注意:
    #    Predict识别接口，仅在ret_code == 0时才会进行扣款，才需要进行退款请求，否则无需进行退款操作
    # 注意2:
    #   退款仅在正常识别出结果后，无法通过网站验证的情况，请勿非法或者滥用，否则可能进行封号处理
    #
    def Justice(self, request_id):
        if request_id == "":
            #
            return
        tm          = str( int(time.time()))
        sign        = CalcSign( self.pd_id, self.pd_key, tm)
        param       = {
                "user_id": self.pd_id,
                "timestamp":tm,
                "sign":sign,
                "request_id":request_id
                }
        url     = self.host + "/api/capjust"
        rsp     = HttpRequest(url, param)
        if rsp.ret_code == 0:
            LOG("justice succ ret: {} request_id: {} pred: {} err: {}".format( rsp.ret_code, rsp.request_id, rsp.pred_rsp.value, rsp.err_msg))
        else:
            LOG("justice failed ret: {} err: {}".format( rsp.ret_code, rsp.err_msg.encode('utf-8')))
        return rsp

    #
    # 充值接口
    # 参数：cardid：充值卡号  cardkey：充值卡签名串
    # 返回值：
    #   rsp.ret_code：正常返回0
    #   rsp.err_msg：异常时返回异常详情
    #
    def Charge(self, cardid, cardkey):
        tm          = str( int(time.time()))
        sign        = CalcSign( self.pd_id, self.pd_key, tm)
        csign       = CalcCardSign(cardid, cardkey, tm, self.pd_key)
        param       = {
                "user_id": self.pd_id,
                "timestamp":tm,
                "sign":sign,
                'cardid':cardid,
                'csign':csign
                }
        url     = self.host + "/api/charge"
        rsp     = HttpRequest(url, param)
        if rsp.ret_code == 0:
            LOG("charge succ ret: {} request_id: {} pred: {} err: {}".format( rsp.ret_code, rsp.request_id, rsp.pred_rsp.value, rsp.err_msg))
        else:
            LOG("charge failed ret: {} err: {}".format( rsp.ret_code, rsp.err_msg.encode('utf-8')))
        return rsp

    ##
    # 充值，只返回是否成功
    # 参数：cardid：充值卡号  cardkey：充值卡签名串
    # 返回值： 充值成功时返回0
    ##
    def ExtendCharge(self, cardid, cardkey):
        return self.Charge(cardid,cardkey).ret_code
    ##
    # 调用退款，只返回是否成功
    # 参数： request_id：需要退款的订单号
    # 返回值： 退款成功时返回0
    #
    # 注意:
    #    Predict识别接口，仅在ret_code == 0时才会进行扣款，才需要进行退款请求，否则无需进行退款操作
    # 注意2:
    #   退款仅在正常识别出结果后，无法通过网站验证的情况，请勿非法或者滥用，否则可能进行封号处理
    ##
    def JusticeExtend(self, request_id):
        return self.Justice(request_id).ret_code
    ##
    # 查询余额，只返回余额
    # 参数：无
    # 返回值：rsp.cust_val：余额
    ##
    def QueryBalcExtend(self):
        rsp = self.QueryBalc()
        return rsp.cust_val
    ##
    # 从文件识别验证码，只返回识别结果
    # 参数：pred_type;识别类型  file_name:文件名
    # 返回值： rsp.pred_rsp.value：识别的结果
    ##
    def PredictFromFileExtend( self, pred_type, file_name, head_info = ""):
        rsp = self.PredictFromFile(pred_type, file_name, head_info)
        return rsp.pred_rsp.value
    ##
    # 识别接口，只返回识别结果
    # 参数：pred_type:识别类型  img_data:图片的数据
    # 返回值： rsp.pred_rsp.value：识别的结果
    ##
    def PredictExtend(self, pred_type, img_data, head_info=""):
        rsp = self.Predict(pred_type, img_data, head_info)
        return rsp.pred_rsp.value
def TestFunc(file_name):
    pd_id = "114874"
    pd_key = "VP8eZUdQQu0/It8AoiQB1T4VS78AkIYd"
    app_id = "314874"
    app_key = "d5XXuMpd6h3HrjNTPs+y2XfJb74QoIwR"
    pred_type = "30400"
    api = FateadmApi(app_id, app_key, pd_id, pd_key)
    # 通过文件形式识别：直接返回识别结果
    return api.PredictFromFileExtend(pred_type, file_name)

if __name__ == "__main__)":
    TestFunc()



西刺免费代理
import urllib.request
import lxml.etree
import telnetlib
import requests
import os
from bs4 import BeautifulSoup
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}
def get_proxy(page_num):
  # 获取页面
  req = urllib.request.Request('http://www.xicidaili.com/nn/{}'.format(page_num), headers=headers) # 构造request请求
  response = urllib.request.urlopen(req) # 发送请求
  html = response.read()
  html = html.decode('utf-8')
  # print(html)
  # 解析页面
  proxy_list = []
  selector = lxml.etree.HTML(html)
  rows = selector.xpath('//*[@id="ip_list"]//tr')
  rows_total = len(rows)
  row_xpath_head = '//*[@id="ip_list"]//tr['
  row_ip_xpath_tail = ']/td[2]/text()'
  row_port_xpath_tail = ']/td[3]/text()'
  for i in range(1, rows_total):
    ip_xpath = row_xpath_head + str(i+1) + row_ip_xpath_tail
    port_xpath = row_xpath_head + str(i+1) + row_port_xpath_tail
    ip = selector.xpath(ip_xpath)[0]
    port = selector.xpath(port_xpath)[0]
    ip_port = ip + ':' + port
    proxy_list.append(ip_port)
  return proxy_list
# 检测代理ip是否可用
def test_proxy_ip_port(proxy_ip_port):
  print('当前代理ip：{}'.format(proxy_ip_port))
  ip_port = proxy_ip_port.split(':')
  ip = ip_port[0]
  port = ip_port[1]
  # 用telnet来验证ip是否可用
  try:
    telnetlib.Telnet(ip, port, timeout=10)
  except:
    return False
  else:
    return True
# 把有效的ip写入本地
def write_ip(proxy_ip):
  with open('./ip.txt', 'a') as f:
    f.write(str(proxy_ip)+'\n')
# 删除文件
def del_file():
  file_path = './ip.txt'
  if os.path.exists(file_path):
    os.remove(file_path)
def run():
  del_file()
  proxy_ip_port_list = []
  for i in range(1, 6): # 前5页
    proxy_ip_port_list += get_proxy(i)
  for i in range(100): # 一页有100条
    proxy_ip_port = proxy_ip_port_list[i]
    is_valid = test_proxy_ip_port(proxy_ip_port)
    print(is_valid)
    if is_valid:
        write_ip(proxy_ip_port)
if __name__ == '__main__':
  run()

爬虫并发：
t1 = threading.Thread(target=company_detailinfo_shareholder, args=(driver, company, companydetailinfo, soup,))
t2 = threading.Thread(target=company_detailinfo_keypersonnel, args=(driver, companydetailinfo, company,))
t3 = threading.Thread(target=company_detailinfo_changerecord, args=(driver, companydetailinfo, company,))
t4 = threading.Thread(target=company_detailinfo_branchstructure, args=(driver, companydetailinfo, company,))
t1.start()
t2.start()
t3.start()
t4.start()
t1.join()
t1.join()
t1.join()
t1.join()


#企业年报
class Annualreport(object):
    def __init__(self):
        self.year=None
        self.websitesOrOnlineStore=None
        # self.sponsors=None
        self.totalAssets=None
        self.totalOwnersEquity=None
        self.grossRevenue=None
        self.totalProfit=None
        self.mainBusinessIncome=None
        self.retainedProfits=None
        self.totalTax=None
        self.totalLiabilities=None
        self.guarantee=None
    def getYear(self):
        return self.year
    def setYear(self, value):
        self.year = value
    def getWebsitesOrOnlineStore(self):
        return self.websitesOrOnlineStore
    def setWebsitesOrOnlineStore(self, value):
        self.websitesOrOnlineStore = value
    # def getSponsors(self):
    #     return self.sponsors
    # def setSponsors(self, value):
    #     self.sponsors = value
    def getTotalAssets(self):
        return self.totalAssets
    def setTotalAssets(self, value):
        self.totalAssets = value
    def getTotalOwnersEquity(self):
        return self.totalOwnersEquity
    def setTotalOwnersEquity(self, value):
        self.totalOwnersEquity = value
    def getGrossRevenue(self):
        return self.grossRevenue
    def setGrossRevenue(self, value):
        self.grossRevenue = value
    def getTotalProfit(self):
        return self.totalProfit
    def setTotalProfit(self, value):
        self.totalProfit = value
    def getMainBusinessIncome(self):
        return self.mainBusinessIncome
    def setMainBusinessIncome(self, value):
        self.mainBusinessIncome = value
    def getRetainedProfits(self):
        return self.retainedProfits
    def setRetainedProfits(self, value):
        self.retainedProfits = value
    def getTotalTax(self):
        return self.totalTax
    def setTotalTax(self, value):
        self.totalTax = value
    def getTotalLiabilities(self):
        return self.totalLiabilities
    def setTotalLiabilities(self, value):
        self.totalLiabilities = value
    def getGuarantee(self):
        return self.guarantee
    def setGuarantee(self, value):
        self.guarantee = value
    def __str__(self):
        return "year: " + self.year+ \
                ",websitesOrOnlineStore: " + self.websitesOrOnlineStore +\
               ",totalAssets: " + self.totalAssets + \
               ",totalOwnersEquity"+self.totalOwnersEquity+\
               ",grossRevenue："+self.grossRevenue+\
               ",totalProfit:"+self.totalProfit+\
               ",mainBusinessIncome："+self.mainBusinessIncome+\
               ",retainedProfits:"+self.retainedProfits+\
               ",totalTax:"+self.totalTax+\
               ",totalLiabilities:"+self.totalLiabilities+\
               ",guarantee:"+self.guarantee